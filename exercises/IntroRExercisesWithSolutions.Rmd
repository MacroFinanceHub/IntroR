---
title: "Introduction to R"
author: "Willi Mutschler"
subtitle: "Solutions to Exercises"
output:
  html_notebook:
    number_sections: yes
    toc: yes
    toc_depth: 2
    toc_float: yes
    code_folding: hide
---

***  
# Introduction
```{r include=FALSE}
rm(list = ls())
```

1. Start ***R-Studio*** and have a look at all menu items.

2. Under `Tools - Options` choose your preferred setting.

3. Install the packages `dplyr`, `ggplot2`, `tidyr`, `stringr`, `readr`, `foreign`, `readxl`, `haven`, `sandwich`, `prettyR`, `Rcmdr`, `xtable`, `texreg`, and `lmtest`
```{r,eval=FALSE}
#you will need to install packages only once
install.packages("dplyr")
install.packages("ggplot2")
install.packages("tidyr")
install.packages("stringr")
install.packages("readr")
install.packages("foreign")
install.packages("readxl")
install.packages("haven")
install.packages("sandwich")
install.packages("prettyR")
install.packages("Rcmdr")
install.packages("xtable")
install.packages("texreg")
install.packages("lmtest")
```
More simply: Use the `Packages` Tab of RStudio to install and activate the libraries.

4. Load the ggplot2 package, list the packages in your memory and detach the ggplot2 package from the memory
```{r}
library("ggplot2")
search()
detach("package:ggplot2")
search()
```


5. The current working directory (where R reads and writes files) can be found by the command _getwd()_. Find your current working directory.
```{r,eval=FALSE}
getwd()
```

6. Use the  command _setwd("c:/path")_ or _setwd(choose.dir())_ (only available on windows) to change the working directory to drive *c:* and path */path*. Note that the path name is structured by slashes (*/*), **not** backslashes (*\\*). Change the working directory to _c:/temp_ and check if the change has been successful. 
```{r,eval=FALSE}
setwd("C:/temp")
#setwd(choose.dir()) # on windows
getwd()
```
Hint: The working directory can also be changed via the menu: _Session -- Set Working Directory_.

7. Open a new script file. Type the commands to perform the following assignments:
\begin{align*}
a &=\frac{3\cdot (4+9)}{8-12.5} \\
b &=\left( 1,4,1999,2011\right) \\
d &=2\pi \\
e &=a+d
\end{align*}
Save the script and quit R.
```{r}
a <- 3 * (4 + 9) / (8 - 12.5)
b <- c(1, 4, 1999, 2011)
d <- 2 * pi
e <- a + d
```

8. Start R and re-open the script. Mark all lines (*Ctrl-A*) and execute them. Print $a,b,d,e.$ 
```{r echo=FALSE}
print(a); print(b); print(d); print(e)
```

9. Why is $c$ not used as a variable?
```{r}
# Because c() is the built-in concatenation command! Do not use c for variable or function declaration. This could mess up your code.
```

***
# Logical Operators
```{r include=FALSE}
rm(list = ls())
```

1. Use the command `c()` to define the vectors
\begin{align*}
x &=\left( -1,0,1,4,9,2,1,4.5,1.1,-0.9\right) \\
y &=\left( 1,1,2,2,3,3,4,4,5,NA\right) .
\end{align*}
```{r}
x <- c(-1, 0, 1, 4, 9, 2, 1, 4.5, 1.1, -0.9)
y <- c(1, 1, 2, 2, 3, 3, 4, 4, 5, NA)
```

2. Determine the lengths of the vectors using `length()` and check if `length(x)==length(y)`.
```{r}
length(x); length(y); length(x) == length(y)
```

3. Perform the following logical operations:
\begin{align*}
x &<y \\
x &<0 \\
x+3 &\geq 0 \\
y &<0 \\
x<0 &\text{ or }y<0
\end{align*}
```{r}
x < y
x < 0
x + 3 >= 0
y < 0
x < 0 | y < 0
```

4. Use `all` to check if all elements of $x+3\geq 0.$
```{r}
all(x + 3 >= 0)
```

5. Use `all` to check if all elements of $y>0.$ Use `any`
to check if at least one element of $y>0.$
```{r}
all(y > 0)
all(y > 0, na.rm = TRUE)
any(y > 0)
```

***
# Arithmetic operators and mathematical functions
```{r include=FALSE}
rm(list = ls())
```

1. Define the vectors
\begin{align*}
x &=\left( -1,0,1,4,9,2,1,4.5,1.1,-0.9\right) \\
y &=\left( 1,1,2,2,3,3,4,4,5,NA\right) .
\end{align*}
and compute $x+y$ and $xy$ and $y/x$.
```{r}
x <- c(-1, 0, 1, 4, 9, 2, 1, 4.5, 1.1, -0.9)
y <- c(1, 1, 2, 2, 3, 3, 4, 4, 5, NA)
```

```{r}
x + y
x * y
y / x
```

2. Compute $\ln (x)$. Determine the length of the result vector.
```{r}
log(x)
length(log(x))
```

3. Use `any` to check if the vector $x$ contains elements
satisfying $\sqrt{x}\geq 2$.
```{r}
any(sqrt(x) >= 2)
```

4. Compute
\begin{align*}
a &=\sum_{i=1}^{10}x_{i} \\
b &=\sum_{i=1}^{10}y_{i}^{2}.
\end{align*}
Use the `na.rm=TRUE` option (**na**-**r**e**m**ove) of
the `sum` command to drop the `NA` in $y$.
```{r}
a <- sum(x)
print(a)
b <- sum(y^2, na.rm = TRUE)
print(b)
```

5. Compute
\begin{align*}
\sum_{i=1}^{10}x_{i}y_{i}^{2}.
\end{align*}
```{r}
sum(x * (y^2), na.rm = TRUE)
```

6. The `sum` command is a convenient way to count the number of elements satisfying a certain condition. Count the number of elements of $x>0$.
```{r,collape=TRUE}
sum(x > 0)
```

7. Predict what the following commands will return:
```{r}
x^y
x^(1/y)
log(exp(y))
y*c(-1,1)
x+c(-1,0,1)
sum(y*c(-1,1),na.rm=TRUE)
```


***
# Matrix functions
```{r include=FALSE}
rm(list = ls())
```

1. Define the matrix
\begin{align*}
X=\left[ 
\begin{array}{lll}
1 & 4 & 7 \\ 
2 & 5 & 8 \\ 
3 & 6 & 9
\end{array}
\right] ,
\end{align*}
print its transpose, its dimensions and its determinant.
```{r}
X <- matrix(c(1,2,3,4,5,6,7,8,9), nrow = 3, ncol = 3)
t(X)
dim(X)
det(X)
```

2. Compute the trace of $X$ (i.e. the sum of its diagonal elements).
```{r}
sum(diag(X))
```


3. Type `diag(X) <- c(7,8,9)` to change the diagonal elements. Compute the eigenvalues of (the new) $X$.
```{r}
diag(X) <- c(7, 8, 9)
eigen(X)
```
Is $X$ positive definite?
```{r}
# Yes, since all eigenvalues are positive.
```

4. Invert $X$ and compute the eigenvalues of $X^{-1}$.
```{r}
solve(X)  # inverts X
eigen(solve(X))
```

5. Define the vector $a=(1,3,2)$ and compute `a*X`, ``a%*%X`, and `X%*%a`.
```{r}
a <- c(1, 3, 2)
a * X
a %*% X
X %*% a
```

6. Compute the quadratic form $a^{\prime }Xa$.
```{r}
t(a) %*% X %*% a  
```

7. Define the matrices
$I=\left[ 
\begin{array}{lll}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1
\end{array}
\right]$,
$Y=\left[ 
\begin{array}{lll}
1 & 4 & 7 \\ 
2 & 5 & 8 \\ 
3 & 6 & 9
\end{array}
\begin{array}{lll}
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1
\end{array}
\right]$ and $Z=\left[ 
\begin{array}{lll}
1 & 4 & 7 \\ 
2 & 5 & 8 \\ 
3 & 6 & 9 \\
1 & 0 & 0 \\ 
0 & 1 & 0 \\ 
0 & 0 & 1
\end{array}
\right]$.
```{r}
I <- diag(3)
Y <- cbind(matrix(1:9,3,3),I)
Z <- rbind(matrix(1:9,3,3),I)
```

8. Predict what the following commands will return:
```{r}
cbind(1,X)
rbind(Y,c(1,2,3))
X%*%I
dim(X%*%Y)
t(Y)+Z
solve(t(Z)%*%Z)%*%(t(Z)%*%Z)
```

***
# Set operations and special functions
```{r include=FALSE}
rm(list = ls())
```

1. Define the vectors
\begin{align*}
x &=\left( -1,0,1,4,9,2,1,4.5,1.1,-0.9\right) \\
y &=\left( 1,1,2,2,3,3,4,4,5,NA\right) .
\end{align*}
and compute $x\cup y$. Determine the lengths of $x$, $y$ and $x\cup y$.
```{r}
x <- c(-1, 0, 1, 4, 9, 2, 1, 4.5, 1.1, -0.9)
y <- c(1, 1, 2, 2, 3, 3, 4, 4, 5, NA)
union(x, y)
length(x)
length(y)
length(union(x, y))
```

2. Count the number of elements of $y$ that are element of $x$.
```{r}
sum(unique(y) %in% x)  
length(intersect(y,x))
```

3. Determine the length of the vector of unique elements of $y$.
```{r}
length(unique(y))
```

4. Compute the vector $z$ with elements
\begin{align*}
z_{i}=\sum_{j=1}^{i}x_{j}
\end{align*}
for $i=1,\ldots ,10$.
```{r}
z <- cumsum(x)
print(z)
```


5. Find the position of the largest element of $x$.
```{r}
which.max(x) 
```

***
# Sequences and replications
```{r include=FALSE}
rm(list = ls())
```

1. Generate the vectors
\begin{align*}
x_{1} &=\left( 1,2,3,\ldots ,9\right) \\
x_{2} &=\left( 0,1,0,1,0,1,0,1\right) \\
x_{3} &=\left( 1,1,1,1,1,1,1,1\right) \\
x_{4} &=\left( -1,1,-1,1,-1,1\right) \\
x_{5} &=\left( 1980,1985,1990,\ldots ,2010\right) \\
x_{6} &=\left( 0,0.01,0.02,\ldots ,0.99,1\right)
\end{align*}
```{r}
x1 <- 1:9  # or c(1:9)
x2 <- rep(c(0, 1), times = 4)
x3 <- rep(1, times = 8)
x4 <- rep(c(-1, 1), times = 3)
x5 <- seq(from = 1980, to = 2010, by = 5)
x6 <- seq(0, 1, by = 0.01)
```

2. Replications can also be generated for vectors of strings
(characters). Type
```{r}
a <- c("a", "b", "c")
rep(a, 3)
rep(a, times = 3)
rep(a, each = 3)
```

3. Generate a grid of $n=500$ equidistant points on the interval $[-\pi,\pi ]$.
```{r,eval=FALSE}
seq(-pi, pi, length = 500)
```

4. Compare `1:10+1` and `1:(10+1)`.
```{r}
1:10 + 1    # sequence from 2 to 11
1:(10 + 1)  # sequence from 1 to 11
```

5. Predict what the following commands will return:
```{r}
rep("bla",10)
rep(rep(1:3,2),each=4)
rep(c(1,6,NA,2),times=c(2,2,5,3))
```


***
# Reading and writing text files
```{r include=FALSE}
rm(list = ls())
```
Consider the files **bsp1.txt**, **bsp2.txt** and **bsp3.txt**. The three files contain computer generated random numbers.

1. Read the file **bsp1.txt** into a data frame `bsp1`. Have a look at the file and the data format _before_ you decide which reading command you use (`read.csv`, `read.csv2` or `read.table`). Print the data frame. If the data frame is too large for your screen, you can use the commands `head` and `tail` to print just parts of it.
```{r}
bsp1 <- read.csv2("data/bsp1.txt")  # German format (sep=';', dec=','), therefore read.csv2
print(bsp1)
head(bsp1)
tail(bsp1)
```

2. Read the files **bsp2.txt** and **bsp3.txt** into data frames `bsp2` and `bsp3`. Note that **bsp2.txt** contains both numeric and character entries. It is usually advisable to set the option `as.is=TRUE` when reading characters (strings).
```{r}
# international format (sep=',', dec='.'), therefore read.csv:
bsp2 <- read.csv("data/bsp2.txt")
# See help file for default settings
bsp3 <- read.table("data/bsp3.txt", dec = ".", sep = ",")
```

3. Print the class of `bsp2`, its dimension, and its variable names (use `names`).
```{r}
class(bsp2)
dim(bsp2)
names(bsp2)
```


4. Print a summary of `bsp3`.
```{r}
summary(bsp3)
```

5. Compute the mean and the standard deviation of each column of `bsp3`.
```{r}
apply(bsp3, 2, sd)
apply(bsp3, 2, mean)
```

6. Create a small data frame `a` with two variables

x | y
--|--
1 | 4
2 | 5 
3 | 6

and write it to a file **smalldataframe.csv** in your working directory.
```{r}
x <- 1:3
y <- 4:6
smalldataframe <- data.frame(x, y)
write.csv2(smalldataframe, file = "data/smalldataframe.csv")  # write.csv2 for German Excel
```

7. Read the (large) file **lest2001.csv** into a data frame `x`. The file is the campus file of the German income tax records 2001 (the data are provided by the Research Data Centre of the Federal Statistical Office, they are described in **lest2001.pdf**). Take care to set the options of the `read.csv` or `read.table` command correctly. The data format is as follows:

* All data entries are separated by semi-colons.
* The first row contains the variables names.
* Missing values are denoted by a dot.
* Apart from the last column all data are integer values.
* The decimal sign in the last column is a dot.

Execute `y <- x\$zve`. The variable `y` now contains the taxable income (**z**u **v**ersteuerndes **E**inkommen). Compute its range, its median, its mean, its variance, and the 0.01- and 0.99-quantiles. Remember to include the option `na.rm=TRUE` in the functions.
```{r}
lest2001 <- read.table("data/lest2001.csv", header = TRUE, na.strings= ".", dec = ".", sep = ";")
head(lest2001)
y <- lest2001$zve
range(y, na.rm = TRUE)
median(y, na.rm = TRUE)
var(y, na.rm = TRUE)
quantile(y, p = c(0.01, 0.99), na.rm = TRUE)
```

8. Import the dataset **swimming_pools.csv**. It contains data on swimming pools in Brisbane, Australia (Source: data.gov.au). The file contains the column names in the first row and uses a comma to separate values within rows.
 
```{r}
pools <- read.csv("data/swimming_pools.csv", stringsAsFactors = FALSE)
str(pools)# Check the structure of pools
```

9. Import hotdogs.txt, containing information on sodium and calorie levels in different hotdogs (Source: UCLA). The dataset has 3 variables (type, calories and sodium), but the variable names are not available in the first line of the file. The file uses tabs as field separators.
```{r}
hotdogs <- read.delim("data/hotdogs.txt", header = FALSE)
summary(hotdogs) # Summarize hotdogs
hotdogs <- read.delim("data/hotdogs.txt", header = FALSE, col.names = c("type", "calories", "sodium"))
str(hotdogs)
# Edit the colClasses argument to import the data correctly: hotdogs2
hotdogs2 <- read.delim("data/hotdogs.txt", header = FALSE, 
                       col.names = c("type", "calories", "sodium"),
                       colClasses = c("factor", "NULL", "numeric"))
str(hotdogs2)
```

***
# Import data using readr
```{r include=FALSE}
rm(list = ls())
```
Use the package `readr` to import the datasets used in the previous exercise, i.e. **bsp1.txt**, **bsp2.txt**, **bsp3.txt**, **lest2001.csv**, **swimming_pools.csv** and **hotdogs.txt**.
```{r}
library(readr)

```


***
# Import Excel data
```{r include=FALSE}
rm(list = ls())
```

1. Load the `readxl` package.
```{r}
library(readxl)
```

2. Print the names of all worksheets in the excel file **urbanpop.xlsx**. This dataset is a subset of the gapminder dataset.
```{r}
excel_sheets("data/urbanpop.xlsx")
```

3. Now read the sheets, one by one, using `read_excel` and put these into a list.
```{r}
pop_1 <- read_excel("data/urbanpop.xlsx", sheet = 1)
pop_2 <- read_excel("data/urbanpop.xlsx", sheet = 2)
pop_3 <- read_excel("data/urbanpop.xlsx", sheet = 3)
pop_list <- list(pop_1, pop_2, pop_3)
pop_list
```

***
```{r include=FALSE}
rm(list = ls())
```

# Import data using `haven`
1. Load the `haven` package.
```{r}
library("haven")
```

2. In this exercise, you will work with data on yearly import and export numbers of sugar, both in USD and in weight. The data is given in **trade.dta**. Load the data using `read_dta` and have a look at the structure. Convert the values in Date column to dates.
```{r}
sugar <- read_dta("data/trade.dta")
str(sugar) # Structure of sugar
sugar$Date <- as.Date(as_factor(sugar$Date))
str(sugar)
```

***
```{r include=FALSE}
rm(list = ls())
```

# Import data using `foreign`
1. Load the `foreign` package.
```{r}
library(foreign)
```

1. In this exercise, you will import data on the US presidential elections in the year 2000. The data in **florida.dta** contains the total numbers of votes for each of the four candidates as well as the total number of votes per election area in the state of Florida. Import **florida.dta** and name the resulting data frame florida.
```{r}
florida <- read.dta("data/florida.dta")
tail(florida)
```

2. The arguments you will use most often are `convert.dates`, `convert.factors`, `missing.type` and `convert.underscore`. Consider the dataset **edequality** which contains socio-economic measures and access to education for different individuals (source: Worldbank).
```{r}
edu_equal_1 <- read.dta("data/edequality.dta")
str(edu_equal_1)
edu_equal_2 <- read.dta("data/edequality.dta", convert.factors = FALSE)
str(edu_equal_2)
edu_equal_3 <- read.dta("data/edequality.dta", convert.underscore = TRUE)
str(edu_equal_3)
```

***
# Indexing vectors
```{r include=FALSE}
rm(list = ls())
```

Define the following vectors
\begin{align*}
x=\left( 
\begin{array}{c}
1 \\ 
1.1 \\ 
9 \\ 
8 \\ 
1 \\ 
4 \\ 
4 \\ 
1
\end{array}
\right) ,\quad y=\left( 
\begin{array}{c}
1 \\ 
2 \\ 
3 \\ 
4 \\ 
4 \\ 
3 \\ 
2 \\ 
NA
\end{array}
\right) ,\quad z=\left( 
\begin{array}{c}
TRUE \\ 
TRUE \\ 
FALSE \\ 
FALSE \\ 
TRUE \\ 
FALSE \\ 
FALSE \\ 
FALSE
\end{array}
\right)
\end{align*}
```{r}
x <- c(1, 1.1, 9, 8, 1, 4, 4, 1)
y <- c(1, 2, 3, 4, 4, 3, 2, NA)
z <- c(TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE)
```

1. Predict what the following commands will return (and then check if you are right):
```{r}
x[-2]
x[2:5]
x[c(1,5,8)]
x[-c(1,5,8)]
x[y]
x[seq(2,8,by=2)]
x[rep(1:3,4)]
```


2. Predict what the following commands will return (and then check if you are right):
```{r}
y[z]
y[!z]
y[x>2]
y[x==1]
x[!is.na(y)]
y[!is.na(y)]
```

3. Indexing is not only used to read certain elements of a vector but also to change them. Execute `x2 <- x` to make a copy of `x`. Change all elements of `x2` that have the value 4 to the value $-4$. Print `x2`.
```{r}
x2 <- x  
x2[x2 == 4] <- -4
print(x2)
```

4. Change all elements of `x2` that have the value 1 to a missing value (`NA`). Print `x2`.
```{r}
x2[x2 == 1] <- NA
print(x2)
```

5. Execute `x2[z] <- 0`. Print `x2`.
```{r}
x2[z] <- 0
print(x2)
```

***
# Indexing matrices
```{r include=FALSE}
rm(list = ls())
```

Define the matrix `x <- matrix(c(1:12,12:1),4,6)`.
```{r}
x <- matrix(c(1:12, 12:1), 4, 6)
```

1. Predict what the following commands will return (and then check if you are right):
```{r}
x[1,3]
x[,5]
x[2,]
x[,-3]
x[-4,]
x[2:3,3:4]
x[2:4,4]
```

2. Predict what the following commands will return (and then check if you are right):
```{r}
x[x>5]
x[,x[1,]<=5]
x[x[,2]>6,]
x[x[,2]>6,4:6]
x[x[,1]<3 & x[,2]<6,]
```

3. Print all rows where column 5 is at least three times larger than column 6.
```{r}
x[x[, 5]>= (3 * x[, 6]) , ]  
```

4. Count the number of elements of `x` that are larger than 7.
```{r}
length(x[x > 7])  # or: sum(x>7)
```

5. Count the number of elements in row 2 that are smaller than their neighbors in row 1.
```{r}
sum(x[2, ] < x[1, ])
```

6. Count the number of elements of `x` that are larger than their left neighbor.
```{r}
sum(x[, 2:6] > x[, 1:5])  # alternativ: sum(x[,-1]>x[,-6])
```

***
# Indexing dataframes
```{r include=FALSE}
rm(list = ls())
```

Load the data set **bsp2.txt** as data frame `bsp2` and print it.
```{r}
bsp2 <- read.csv("data/bsp2.txt",as.is=TRUE)
print(bsp2)
```

1. Use different ways to print the second column of the data frame `bsp2` (as a vector or a data frame).
```{r,,eval=FALSE}
bsp2[, 2]
bsp2$Y
bsp2[[2]]
bsp2["Y"]
```

2. Use different ways to print columns $U$ and $V$.
```{r,,eval=FALSE}
bsp2[, 4]
bsp2$U
bsp2[[4]]
bsp2["U"]
bsp2[, 5]
bsp2$V
bsp2[[5]]
bsp2["V"]
```

3. Use the `attach` command to make the variables directly accessible. Print `X`. Now `detach` the data frame again.
```{r,eval=FALSE,}
attach(bsp2)
print(X)  # For safety reasons apply rm(list = ls()) upfront
detach(bsp2)
```

4. Print all rows of `bsp2` where the variable $U$ has value A or B.
```{r}
bsp2[bsp2$U == "A" | bsp2$U == "B", ]
```

5. Print all rows of `bsp2` where the variable $X$ is smaller than its median and the variable $Y$ is larger than its median.
```{r}
bsp2[bsp2$X < median(bsp2$X) & bsp2$Y > median(bsp2$Y), ]
```

6. One can add row names to a data frame. Execute the following command and print the data frame to have a look at the new row names:
```{r}
row.names(bsp2) <- paste(rep(LETTERS[1:20], each = 2), rep(1:2, 20), sep = "")
print(bsp2)
```

7. Use the row name and the variable name to print the value of variable `Z` at observation `T1`.
```{r}
bsp2["T1", "Z"]
```

8. Print the rows for observations `G1` and `G2`.
```{r}
bsp2[c("G1", "G2"), ]
```


***
# Selection and transformation with `dplyr`
```{r include=FALSE}
rm(list = ls())
```
1. Load `dplyr` and `gapminder` package which will provide you with the **gapminder** dataset. How many observations and variables are in the dataset?
```{r}
library("dplyr")
library("gapminder")
gapminder
```

2. The `filter` verb extracts particular observations based on a condition. Use pipes (`%>%`) to select all information of the year 2007. For how many countries is there data available?
```{r}
gapminder %>% 
  filter(year == 2007)
```

3. Now choose all the data for the United States in the year 2007 using the `filter` command. How many observations do you get?
```{r}
gapminder %>% 
  filter(year == 2007, country == "United States")
```

4. `arrange` is a useful command for sorting data frames. First, sort the data frame by gdp per capita in ascending order. Second, sort the data set by gdp per capita in descending order for the year 2007 only.
```{r}
gapminder %>% 
  arrange(gdpPercap)
gapminder %>%
  filter(year == 2007) %>%
  arrange(desc(gdpPercap))
```

5. `mutate` is useful whenever you want to change or add variables to your dataset. First, replace the variable `pop` (population) by dividing it by 1000000. Second, add a new variable **gdp** for total gross domestic product.
```{r}
gapminder %>%
  mutate(pop = pop/1000000)   #change variable
gapminder %>%
  mutate(gdp = gdpPercap*pop) #add new variable total gdp
```

6. Which countries have the highest gdp in 2007?
```{r}
gapminder %>%
  mutate(gdp = gdpPercap*pop) %>%
  filter(year == 2007) %>%
  arrange(desc(gdp))
```

7. The basic use of the `summarize` verb is to turn many rows into one. Use it to output the mean and median of `lifeExp` as well as the total population in 2007 into a new data frame using pipes.
```{r}
gapminder %>%
  filter(year==2007) %>%
  summarize(meanLifeExp = mean(lifeExp), medianLifeExp = median(lifeExp), totalPop = sum(as.numeric(pop)))
```

8. Now do the same, but for all years, using `group_by(year)`.
```{r}
gapminder %>%
  group_by(year) %>%
  summarize(meanLifeExp = mean(lifeExp), medianLifeExp = median(lifeExp), totalPop = sum(as.numeric(pop)))
```

9. Again get the same statistics, but this time by continent for the year 2007.
```{r}
gapminder %>%
  filter(year==2007) %>%
  group_by(continent) %>%
  summarize(meanLifeExp = mean(lifeExp), medianLifeExp = median(lifeExp), totalPop = sum(as.numeric(pop)))
```

10. Lastly, get the same statistics by continent and year
```{r}
gapminder %>%
  group_by(year,continent) %>%
  summarize(meanLifeExp = mean(lifeExp), medianLifeExp = median(lifeExp), totalPop = sum(as.numeric(pop)))
```


***
# Graphics with `ggplot`
```{r include=FALSE}
rm(list = ls())
```
1. Load `dplyr`, `ggplot2` and `gapminder` package which will provide you with the **gapminder** dataset. How many observations and variables are in the dataset?
```{r}
library("dplyr")
library("ggplot2")
library("gapminder")
```

2. Save all data from 2007 into the data frame **gapminder_2007**.
```{r}
gapminder_2007 <- gapminder %>%
  filter(year == 2007)
```

3. Visualize countries wealth (`gdpPercap` on x axis) against life expectancy (`lifeExp` on y axis) using the good old `plot` command. Compare this to the way ggplot draws a scatterplot when using `geom_point`.
```{r}
plot(gapminder_2007$gdpPercap,gapminder_2007$lifeExp)
ggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp)) +
  geom_point()
```

4. Now add a log scale `scale_x_log10` for gdpPercap to your ggplot scatterplot.
```{r}
ggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp)) +
  geom_point() +
  scale_x_log10() # # each unit on the x-axis represents a change of 10 times the gdp
```

5. Create a scatter plot comparing `pop` and `gdpPercap` for the year 2007, with both axes on a log scale.
```{r}
ggplot(gapminder_2007, aes(x = pop, y = gdpPercap)) +
  geom_point() +
  scale_x_log10() +
  scale_y_log10()
```

6. To add more variables to a 2-dimensional plot, we can use two more asthetics: color for a categorial variable and size for numerical variables. Add the continent and pop to the scatterplot of `gdpPercap` and `lifeExp` for the year 2007
```{r}
ggplot(gapminder_2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +
  geom_point() +
  scale_x_log10()
```

7. Now we want to compare the dynamic relationship between `gdpPercap` and `lifeExp` for all years. Use `facet_wrap(~ year)` on the original dataset **gapminder** to add a facet to your scatterplot.
```{r}
ggplot(gapminder, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +
  geom_point() +
  scale_x_log10() +
  facet_wrap(~ year)
```

8. Create a data frame from the **gapminder** dataset with summarized data with the mean of `lifeExp` and total population, both grouped by year. Visualize this summarized data using `ggplot` and let your y axis begin at 0.
```{r}
by_year <- gapminder %>%
  group_by(year) %>%
  summarize(meanLifeExp = mean(lifeExp), totalPop = sum(as.numeric(pop)))
ggplot(by_year, aes(x = year, y = totalPop)) +
  geom_point() + 
  expand_limits(y=0) # start y axis at 0
```

9. Create a data frame from the **gapminder** dataset with summarized data with the mean of `lifeExp` and total population, both grouped by year and continent. Visualize this summarized data using `ggplot` and let your y axis begin at 0.
```{r}
by_year_continent <- gapminder %>%
  group_by(year,continent) %>%
  summarize(meanLifeExp = mean(lifeExp), totalPop = sum(as.numeric(pop)))
ggplot(by_year_continent, aes(x = year, y = meanLifeExp, color = continent)) +
  geom_point() + 
  expand_limits(y=0) # start y axis at 0
```

10. Create a line plot (`geom_line`) with `ggplot` for the just created subset of data grouped by year and continent. Put year on the x axis, meanLifeExp on the y axis and let the color indicate the continents.
```{r}
ggplot(by_year_continent, aes(x = year, y = meanLifeExp, color = continent)) +
  geom_line() + 
  expand_limits(y=0) # start y axis at 0
```

11. Filter the original **gapminder** data for the year 2007, group by continent and summarize the mean life expectancy for this data frame. Create a bar plot (`geom_col`) with ggplot.
```{r}
by_continent <- gapminder %>%
  filter(year == 2007) %>%
  group_by(continent) %>%
  summarize(meanLifeExp = mean(lifeExp))
#x is categorial variable
ggplot(by_continent, aes(x = continent, y = meanLifeExp, color = continent)) +
  geom_col()
```

12. Create a histogram (`geom_histogram`) of population (pop) in 2007 using a log scale.
```{r}
gapminder_2007 <- gapminder %>%
  filter(year == 2007)

ggplot(gapminder_2007, aes(x = pop)) +
  geom_histogram() + 
  scale_x_log10()
```

13. Create a boxplot (`geom_bloxplot`) comparing gdpPercap among continents for the year 2007 and add a title to the graph using `ggtitle`.
```{r}
gapminder_2007 <- gapminder %>%
  filter(year == 2007)
ggplot(gapminder_2007, aes(x = continent, y = gdpPercap)) +
  geom_boxplot() +
  scale_y_log10() +
  ggtitle("Comparing GDP per capita across continents (log-scale)")
```




***
# Histograms
```{r,include=FALSE}
rm(list = ls())
```
In this section, please always use the command `truehist` (which is included in the `MASS` package) to generate histograms.
```{r}
library(MASS)
```

1. Load the file **gemeinden2006.csv** into a data frame. Delete all observations where the number of inhabitants (`Einwohner`) is smaller than 5. Plot the histogram of the logarithm of the variable `Einwohner.
```{r}
gemeinden2006 <- read.csv2("data/gemeinden2006.csv")
gemeinden2006new <- gemeinden2006[gemeinden2006$Einwohner >= 5, ] #or x[!x$Einwohner < 5, ]
truehist(log(gemeinden2006new$Einwohner), col = "lightblue")
```

2.Add the density function of a fitted normal distribution to the histogram.
```{r}
truehist(log(gemeinden2006new$Einwohner), col = "lightblue")
m <- mean(log(gemeinden2006new$Einwohner))
s <- sd(log(gemeinden2006new$Einwohner))
x <- seq(1, 15, length = 500)
lines(x, dnorm(x, mean = m, sd = s), lwd = 2)
```

3. Load the Stata file **mikrozensus2002cf.dta** into a data frame. Consider the variable `ef462` (rent in April 2002). Drop all observations where the rent exceeds 2000 Euro. Plot the histogram.
```{r}
library(foreign)
mikrozensus2002cf <- read.dta("data/mikrozensus2002cf.dta")
y <- mikrozensus2002cf$ef462[!is.na(mikrozensus2002cf$ef462)]  
yy <- y[y <= 2000]  
truehist(yy, col = "pink", xlab = "rent", ylab = "density", main = "histogram of rents")
```

4. Load the Stata file **mikrozensus2002cf.dta** into a data frame.

+ Plot the histogram of the variable `ef453` (size of flat in square meters).

+ Drop all observations with more than $300$ $m^{2}$ and plot the histogram again.

+ Set the number of bins in the histogram to 15.
```{r}
truehist(mikrozensus2002cf$ef453, col = "lightblue", xlab = "size of flat (in qm)", ylab = "density")
truehist(mikrozensus2002cf$ef453[mikrozensus2002cf$ef453 <= 300], col = "lightgreen", xlab = "size of flat (in qm)", ylab = "density")
truehist(mikrozensus2002cf$ef453[mikrozensus2002cf$ef453 <= 300], col = "steelblue", nbins = 15, xlab = "size of flat (in qm)", ylab = "density")  # only use 15 classes
```

***
# Correlation and covariance
```{r,include=FALSE}
rm(list = ls())
```

1. Execute `data(Titanic)` to load the object `Titanic` of class `table`. Print it as an ordinary table and as a flat table. Plot it as well. Compute the univariate marginal distributions using the `apply` command. Compute the bivariate marginal distribution of survival and social class (again using `apply`).
```{r}
data(Titanic)
Titanic
table(Titanic)
ftable(Titanic)
ftable(Titanic, row.vars = c("Survived", "Age"))  # Write 'Survived' and 'Age' into rows
plot(Titanic)
apply(Titanic, 1, sum)
apply(Titanic, 2, sum)
apply(Titanic, 3, sum)
apply(Titanic, 4, sum)
apply(Titanic, c(1, 4), sum)
# or margin.table(Titanic,c(1,4))
```

2. Load the file **covmat.csv** into a data frame.
```{r}
covmat <- read.csv("data/covmat.csv")
```

+ Compute the covariance matrix using the option `use="complete"`. Check if the covariance matrix is positive definite.
```{r}
cov(covmat)
cov(covmat,use="complete")
if (sum(eigen(cov(covmat,use="complete"))$value>0) == dim(covmat)[2]){
  print("Matrix is positive definite")
} else {
  print("Matrix is not positive definite")
}
```

+ Now compute the covariance using the option `pairwise` and check again, if the covariance matrix is positive definite.
```{r}
cov(covmat,use="pairwise")
if (sum(eigen(cov(covmat,use="pairwise"))$value>0) == dim(covmat)[2]){
  print("Matrix is positive definite")
} else {
  print("Matrix is not positive definite")
}
```


***
# Cleaning data
```{r include=FALSE}
rm(list = ls())
```
We will consider historical weather data for Boston, USA for 12 months beginning in December 2014. 

1. Load the packages `tidyr`, `dplyr`, `lubridate`, and `stringr`. Import the data using `weather <- readRDS("weather.rds")` and have a look how **dirty** it is.
```{r}
library("tidyr"); library("dplyr"); library("lubridate"); library("stringr")
weather <- readRDS("data/weather.rds")
```


2. The first step is to understand the structure of your data with `class`, `dim`, `names`, `str`, `glimpse`, and `summary`. Also preview the first and last 15 observations with `head` and `tail`.
```{r}
class(weather) # Verify that weather is a data.frame
dim(weather) # Check the dimensions
names(weather) # View the column names
str(weather) # View the structure of the data
glimpse(weather) # Look at the structure using dplyr's glimpse()
summary(weather) # View a summary of the data
head(weather,15)
tail(weather,15)
```

3. The **weather** dataset suffers from one of the five most common symptoms of messy data: column names are values. In particular, the column names `X1`-`X31` represent days of the month, which should really be values of a new variable called `day`. The `tidyr` package provides the `gather()` function for exactly this scenario. Notice that `gather()` allows you to select multiple columns to be gathered by using the `:` operator. Call `gather()` on the weather data to gather columns `X1`-`X31`. The two columns created as a result should be called `day` and `value`. Save the result as weather2 and view it with `head`.
```{r}
weather2 <- gather(weather, day, value, X1:X31, na.rm = TRUE)
head(weather2)
```


4. Our data suffer from a second common symptom of messy data: values are variable names. Specifically, values in the measure column should be variables (i.e. column names) in our dataset. The `spread()` function from `tidyr` is designed to help with this. Remove the first column of weather2, assigning to `without_x`. Spread the measure column of `without_x` and save the result to weather3. View the result with `head()`.
```{r}
without_x <- weather2[, -1] # First remove column of row names
weather3 <- spread(without_x, measure, value) # Spread the data
head(weather3)
```

5. A good package and function to tidy up dates into the same format is `lubridate`, e.g. try out this code
#Dates with lubridate for most common combinations
```{r}
ymd("2015-08-25")
ymd("2015 August 25")
mdy("August 25, 2015")
hms("13:33:09")
ymd_hms("2015/08/25 13.33.09")
```
We'll start by combining the year, month, and day columns and recoding the resulting character column as a date. We can use a combination of `stringr`, and `lubridate` to accomplish this task. 

+ Use `stringr`'s `str_replace()` to remove the `X`s from the `day` column of `weather3`. 

+ Create a new column called `date`. Use the `unite()` function from `tidyr` to paste together the `year`, `month`, and `day` columns in order, using `-` as a separator. 

+ Coerce the `date` column using the appropriate function from `lubridate`. 

+ Use select() to reorder columns, saving the result to `weather5`. 

+ View the head of `weather5`.

```{r}
weather3$day <- str_replace(weather3$day, "X", "") # Remove X's from day column
weather4 <- unite(weather3, date, year, month, day, sep = "-") # Unite the year, month, and day columns
weather4$date <- ymd(weather4$date) # Convert date column to proper date format using lubridates's ymd()
weather5 <- select(weather4, date, Events, CloudCover:WindDirDegrees) # Rearrange columns using dplyr's select()
head(weather5) # View the head of weather5
```

6. Let's look closer at the column types as it is important that variables are coded appropriately for further statistical analysis. This is not yet the case with our weather data. Recall that functions such `as as.numeric()` and `as.character()` can be used to coerce variables into different types. 

+ Use `str()` to see how variables are stored in `weather5`.

+ View the first 20 rows of weather5. Keep an eye out for strange values!

+ Try coercing the `PrecipitationIn` column of `weather5` to numeric without saving the result.

```{r}
str(weather5) # View the structure of weather5
head(weather5, 20) # Examine the first 20 rows of weather5. Are most of the characters numeric?
as.numeric(weather5$PrecipitationIn) # See what happens if we try to convert PrecipitationIn to numeric
```
Scroll the output, notice the warning message. Go back to the results of the head command if need be. What values in PrecipitationIn would become NA if coerced to numbers? Why would they be in the dataset to begin with?

7. As you saw in the last exercise, `T` was used to denote a trace amount (i.e. too small to be accurately measured) of precipitation in the `PrecipitationIn` column. In order to coerce this column to numeric, you'll need to deal with this somehow. To keep things simple, we will just replace `T` with zero, as a string ("0"). 

+ Use `str_replace()` from `stringr` to make the proper replacements in the `PrecipitationIn` column of `weather5`.

+ Run the call to mutate_at to conveniently apply `as.numeric()` to all columns from `CloudCover` through `WindDirDegrees` (reading left to right in the data), saving the result to `weather6`.

+ View the structure of weather6 to confirm the coercions were successful.

```{r}
weather5$PrecipitationIn <- str_replace(weather5$PrecipitationIn, "T", "0") # Replace "T" with "0" (T = trace)
weather6 <- mutate_at(weather5, vars(CloudCover:WindDirDegrees), funs(as.numeric)) # Convert characters to numerics
str(weather6) # Look at result
```

8. Before dealing with missing values in the data, it's important to find them and figure out why they exist in the first place. If your dataset is too big to look at all at once, like it is here, remember you can use `sum()` and `is.na()` to quickly size up the situation by counting the number of NA values. The `summary()` function may also come in handy for identifying which variables contain the missing values. Finally, the `which()` function is useful for locating the missing values within a particular column.

+ Use `sum()` and `is.na()` to count the number of `NA` values in `weather6`.

+ Look at a `summary()` of `weather6` to figure out how the missings are distributed among the different variables.

+ Use `which()` to identify the indices (i.e. row numbers) where `Max.Gust.SpeedMPH` is `NA` and save the result to `ind`.

+ Use `ind` to look at the full rows of `weather6` for which `Max.Gust.SpeedMPH` is missing.

```{r}
sum(is.na(weather6)) # Count missing values
summary(weather6) # Find missing values
ind <- which(is.na(weather6$Max.Gust.SpeedMPH)) # Find indices of NAs in Max.Gust.SpeedMPH
weather6[ind, ] # Look at the full rows for records missing Max.Gust.SpeedMPH
```

9. Besides missing values, we want to know if there are values in the data that are too extreme or bizarre to be plausible. A great way to start the search for these values is with `summary()`. Once implausible values are identified, they must be dealt with in an intelligent and informed way. Sometimes the best way forward is obvious and other times it may require some research and/or discussions with the original collectors of the data.

+ View a `summary()` of `weather6`.

+ Use `which()` to find the index of the erroneous element of `weather6$Max.Humidity`, saving the result to `ind`.

+ Use `ind` to look at the full row of `weather6` for that day. You discover an extra zero was accidentally added to this value. Correct it in the data.
```{r}
summary(weather6) # Review distributions for all variables
ind <- which(weather6$Max.Humidity == 1000) # Find row with Max.Humidity of 1000
weather6[ind, ] # Look at the data for that day
weather6$Max.Humidity[ind] <- 100 # Change 1000 to 100
```

10. You've discovered and repaired one obvious error in the data, but it appears that there's another. Sometimes you get lucky and can infer the correct or intended value from the other data. For example, if you know the minimum and maximum values of a particular metric on a given day.

+ Use `summary()` to look at the value of only the `Mean.VisibilityMiles` variable of `weather6`.

+ Determine the element of the value that is clearly erroneous in this column, saving the result to `ind`.

+ Use `ind` to look at the full row of `weather6` for this day.

+ Inspect the values of other variables for this day to determine the correct value of `Mean.VisibilityMiles`, then make the appropriate fix.

```{r}
summary(weather6$Mean.VisibilityMiles) # Look at summary of Mean.VisibilityMiles
ind <- which(weather6$Mean.VisibilityMiles == -1) # Get index of row with -1 value
weather6[ind, ] # Look at full row
weather6$Mean.VisibilityMiles[ind] <- 10 # Set Mean.VisibilityMiles to the appropriate value
```

11. In addition to dealing with obvious errors in the data, we want to see if there are other extreme values. In addition to the trusty `summary()` function, `hist()` is useful for quickly getting a feel for how different variables are distributed.

+ Check a `summary()` of `weather6` one more time for extreme or unexpected values.

+ View a histogram for `MeanDew.PointF`, `Min.TemperatureF` and `Mean.TemperatureF` to compare distributions.

```{r}
summary(weather6) # Review summary of full data once more
hist(weather6$MeanDew.PointF) # Look at histogram for MeanDew.PointF
hist(weather6$Min.TemperatureF) # Look at histogram for Min.TemperatureF
hist(weather6$Mean.TemperatureF) # Compare to histogram for Mean.TemperatureF
```



12. Finally, the `Events` column contains an empty string ("") for any day on which there was no significant weather event such as rain, fog, a thunderstorm, etc. However, if it's the first time you're seeing these data, it may not be obvious that this is the case, so it's best for us to be explicit and replace the empty strings with something more meaningful. Replace all empty strings in the events column of weather6 with "None". One last time, print out the first 6 rows of the weather6 data frame to see the changes.

```{r}
weather6$Events[weather6$Events == ""] <- "None" # Replace empty cells in events column
head(weather6) # Print the first 6 rows of weather6
```


***
# User-defined functions
```{r include=FALSE}
rm(list = ls())
```

1. Define a Cobb-Douglas production function with two inputs vectors,
\begin{align*}
x &=\left( 
\begin{array}{c}
L \\ 
K
\end{array}
\right) \\
\theta &=\left( 
\begin{array}{c}
A \\ 
\alpha \\ 
\beta
\end{array}
\right)
\end{align*}
and scalar output
\begin{align*}
y=AL^{\alpha }K^{\beta }.
\end{align*}
Evaluate the function at
\begin{align*}
x &=\left( 
\begin{array}{c}
2 \\ 
3
\end{array}
\right) \\
\theta &=\left( 
\begin{array}{c}
1 \\ 
0.3 \\ 
0.8
\end{array}
\right) .
\end{align*}
```{r}
cobb_douglas <- function(x, theta) {
  y <- theta[1] * x[1]^theta[2] * x[2]^theta[3]  
  return(y)
}
cobb_douglas(x = c(2, 3), theta = c(1, 0.3, 0.8))  
```

2. Define a function `lowdecile` with one input vector $\left(x_{1},\ldots ,x_{n}\right)$ of arbitrary length. The function should compute and return the mean of all observations in the lowest decile. Define the vector
\begin{align*}
x=\left( 0,0,0,0,1,1,1,1,2,2,2,2,\ldots ,9,9,9,9\right)
\end{align*}
and apply `lowdecile` to $x$.
```{r}
lowdecile <- function(x) {
  quantil <- x[x <= quantile(x, p = 0.1)]  
  return(mean(quantil))  
}
lowdecile(x = rep(0:9, each = 4))
```


***
# Programming
```{r,include=FALSE}
rm(list = ls())
```
1. This exercise illustrates that loops are often not very efficient.

* Create the vector $x=(1,2,\ldots ,1\,000\,000)$ and convert it from _integer_ to _numeric_ using the conversion command `as.numeric`.
```{r}
x <- 1:1e+06
class(x)
x <- as.numeric(x)
class(x)
```

* Write a `for`-loop to compute the sum of all vector elements without using the `sum` command. Put the command `p0 <- proc.time()[3]` in front of the loop and the command `print(proc.time()[3]-p0)` at the end. These commands allow to measure the execution time of the loop.
```{r}
S <- 0  # initialize
p0 <- proc.time()[3]  #Startzeitpunkt festlegen
for (i in x) {
    S <- S + i
}
print(S)
print(proc.time()[3] - p0)  #time used 
```

*Compare your result with the execution time of the `sum` command.

```{r}
p0 <- proc.time()[3]
sum(x)
print(proc.time()[3] - p0)
```


2. Create a grid vector $x$ of 60 equidistant points $x_{1},\ldots,x_{60} $ on the interval $[-10,10]$, and another grid vector $y$ of 70 points $y_{1},\ldots ,y_{70}$ on $[-10,10]$. Create an empty matrix $Z$ of dimension $60\times 70$.

Write a double loop to compute the matrix elements
\begin{align*}
Z_{ij}=\frac{10}{r_{ij}}\cdot \sin (r_{ij})
\end{align*}
where $r_{ij}=\sqrt{x_{i}^{2}+y_{j}^{2}}$. Execute `persp(x,y,Z)`.
```{r}
x <- seq(-10, 10, length = 60)
y <- seq(-10, 10, length = 70)
Z <- matrix(NA, 60, 70)
for (i in 1:length(x)) {
    for (j in 1:length(y)) {
        r <- sqrt(x[i]^2 + y[j]^2) # note that r is overwritten in each run of the loop
        Z[i, j] <- 10/r * sin(r)
    }
}
persp(x, y, Z, ticktype = "detailed", col = "lightblue")
```

3. Load the data set **fussballdaten.csv**. It contains all _1. Bundesliga_ results between the
seasons 1996/1997 and 2008/2009.
```{r}
fussballdaten <- read.csv2("data/fussballdaten.csv", as.is = TRUE)
```

* Create an alphabetically ordered vector of all clubs in the data set.
```{r}
home <- fussballdaten$Heim
away <- fussballdaten$Auswaerts
clubs <- sort(unique(home))
```

* Write a loop over all clubs. For each club compute the proportion of games won.
```{r}
ngames <- dim(fussballdaten)[1] # number of games in dataset
GoalsH <- fussballdaten$ToreH
GoalsA <- fussballdaten$ToreA
winner <- rep(NA, ngames)
propwin <- rep(NA, length(clubs))
# Get winning teams
for (i in 1:ngames) {
    if (GoalsH[i] > GoalsA[i]) {
        winner[i] <- home[i]
    }
    if (GoalsA[i] > GoalsH[i]) {
        winner[i] <- away[i]
    }
    if (GoalsH[i] == GoalsA[i]) {
        winner[i] <- "Remis"
    }
}
# Get proportions
for (i in 1:length(clubs)) {
    win <- sum(winner == clubs[i])  # Games won by club i
    tot <- sum(home == clubs[i] | away == clubs[i])  # Number of mathes of club i
    propwin[i] <- win/tot
}
names(propwin) <- clubs
print(propwin)
```

* Order the clubs descendingly according to the proportion of games won and plot a `barplot` of the proportion.
```{r}
sort(propwin, decreasing = TRUE)
barplot(sort(propwin, decreasing = TRUE), col = "steelblue", las = 3)  # label is printed vertically using las=3
```


***
# Random numbers
```{r, include=FALSE}
rm(list = ls())
```
This section is not only about random number generation but also includes exercises about the R-functions for standard distributions in statistics.

1. Let's consider a simple count data example.
+ Let $X\sim N\left( 0,1\right) $. Compute the probability $P(|X|>3.5)$.
```{r}
2 * (1 - pnorm(3.5))
```

+ Generate $n=10000$ random draws $X_{1},\ldots ,X_{n}$ from $X$ and count the number of observations $|X_{i}|>3.5$.
```{r}
n <- 10000
X <- rnorm(n)
sum(abs(X) > 3.5)
sum(abs(X) > 3.5)/n
# the larger n the closer it is to the theoretical value of 0.0004652582
```

+ Repeat drawing random samples $R=5000$ times and write the counts into a vector $Z_{1},\ldots ,Z_{5000}$ of length 5000.
```{r}
R <- 5000  
Z <- rep(NA, R)  
n <- 10000
for (i in 1:R) {
    X <- rnorm(n)
    Z[i] <- sum(abs(X) > 3.5)
}
```

+ Tabulate $Z$ and compare the frequencies with the probability function of a suitably fitted Poisson distribution.
```{r}
table(Z)
t(data.frame(observ = 0:16 , prob = dpois(0:16, lambda=mean(Z))*R))
library(MASS)
truehist(Z, prob = F)  
x <- seq(0, 16)
lines(x, dpois(x, lambda = mean(Z)) * R, lwd = 2)  
```


2. Generate $n=10000$ draws from a log normal distribution $X\sim e^{Y}$ where $Y\sim N(1,0.5^{2})$ (the parameters in the R function are `meanlog=1` and `sdlog=0.5`). Split the screen into two plotting areas using the command `par(mfrow=c(2,1))`. Plot the histograms of $X$ and $\ln X$.
```{r}
n <- 10000
x <- rlnorm(n, meanlog = 1, sdlog = 0.5)
par(mfrow = c(2, 1))
truehist(x)
truehist(log(x))
```

3. Generate $n=10000$ draws from $X\sim N(0,1)$. Compute the cumulated means, i.e.
\begin{align*}
\bar{X}_{j}=\frac{1}{j}\sum_{i=1}^{j}X_{i}
\end{align*}
for $j=1,\ldots ,n$ and plot them. Hint: Use the command `cumsum`.
```{r}
par(mfrow = c(1, 1))
n <- 10000
X <- rnorm(n)  
m <- rep(NA, n)
for (i in 1:n) {
    m[i] <- cumsum(X)[i]/i
}
plot(m)
```

***
# Simulations
```{r,include=FALSE}
rm(list = ls())
```
1. This exercise illustrates the one-sample $t$-test.

+ Generate $n=10$ observations from $X\sim N(10,3^{2})$. Compute the mean and the standard deviation of $X_{1},\ldots ,X_{10}$.
```{r}
n <- 10
X <- rnorm(n, mean = 10, sd = 3)
m <- mean(X)
s <- sd(X)
print(m)
print(s)
```

+ The $t$-statistics of the hypothesis test $H_{0}:\mu =10$ against $H_{1}:\mu \neq 10$ is \begin{align*}
t=\sqrt{10}\frac{\bar{X}-10}{sd}
\end{align*}
where $sd$ is the standard deviation (as computed by `sd`). Compute the $t$-statistic.
```{r}
t <- sqrt(n) * (m - 10)/s
print(t)
```

+ Create an empty vector $Z$ of length $R=5000$. Write a loop over $r=1,\ldots ,R$ and repeat the above steps for each $r$. Save the $t$-statistic at $Z_{r}$.
```{r}
R <- 5000
Z <- rep(NA, R)
for (r in 1:R) {
    X <- rnorm(n, mean = 10, sd = 3)
    m <- mean(X)
    s <- sd(X)
    Z[r] <- sqrt(n) * (m - 10)/s
}
```

+ Plot the histogram of $Z_{1},\ldots ,Z_{R}$ and add the density function of the $t_{9}$-distribution.
```{r}
library(MASS)
truehist(Z, col = "lightblue")
x <- seq(-4, 4, by = 0.1)
lines(x, dt(x, df = 9), lwd = 2)
```

2. The classical central limit theorem states that the standardized sum of i.i.d. random variables with finite variance converges in distribution to the standard normal distribution $N(0,1)$. This exercise illustrates the central limit theorem. 

+ Write a simulation that performs the following steps:

* Generate a random sample $X_{1},\ldots ,X_{5}$ of size $n=5$ from the standard exponential distribution $Exp(1)$.

* Compute the sample sum.

* Repeat the steps $R=10\,000$ times. For each replication, store the sum, e.g. into a vector $Z$.

* Plot the histogram of the sum and add the density function of $N(m,s^{2})$ where $m$ is the mean of $Z$ and $s$ is the standard deviation of $Z$.
```{r}
clt_exp <- function(n) {
  R <- 10000
  Z <- rep(NA, R)
  for (r in 1:R) {
      X <- rexp(n, rate = 1)
      Z[r] <- sum(X)
  }
  truehist(Z, col = "lightblue", main=paste("n =",n,sep=" "))
  coord <- par("usr")
  # par("usr") gives you a vector of the form c(x1, x2, y1, y2)
  # giving the extremes of the coordinates of the plotting region
  x <- seq(coord[1], coord[2], by = 0.1)
  lines(x, dnorm(x, mean = mean(Z), sd = sd(Z)), lwd = 2)
}
clt_exp(5)  
```


+ Increase the sample size $n$ to $n=50,500,5000$ and redo the exercise.
```{r}
clt_exp(50)
clt_exp(500)
clt_exp(5000)
```


+ Redo the exercise with other distributions than the exponential. Use the uniform distribution, the $t$-distribution with 3 degrees of freedom, the Bernoulli distribution (i.e.
binomial with parameter size=1), and the Poisson distribution.
```{r}
clt <- function(n, distrib, df=3, lambda=5, prob=0.6) {
  R <- 10000
  Z <- rep(NA, R)
  for (r in 1:R) {
      if (distrib == 1){
        X <- runif(n)
        strdist <- "Uniform"
        }
      if (distrib == 2){
        X <- rt(n, df = df)
        strdist <- "Student''s t"
        }
      if (distrib == 3){
        X <- rbinom(n, size=1, prob=prob)
        strdist <- "Bernoulli"
        }
      if (distrib == 4){
        X <- rpois(n, lambda = lambda)
        strdist <- "Poisson"
        }
      Z[r] <- sum(X)
  }
  truehist(Z, col = "lightblue", xlab = strdist, main = paste("n =", n, sep = " "))
  coord <- par("usr")
  # par("usr") gives you a vector of the form c(x1, x2, y1, y2)
  # giving the extremes of the coordinates of the plotting region
  x <- seq(coord[1], coord[2], by = 0.1)
  lines(x, dnorm(x, mean = mean(Z), sd = sd(Z)), lwd = 2)
}
```

```{r}
par(mfrow = c(2,2))
for (n in c(5,50,500,5000)) {
  for (i in 1:4) {
    clt(n,i)
  }
}
```


+ The central limit theorem breaks down if the variance of the summands is infinite. Redo the exercise using a $t$-distribution with only 1.5 degrees of freedom.
```{r}
par(mfrow = c(1,1))
clt(500, 2, df = 1.5)
```

***
# Linear regression
```{r,include=FALSE}
rm(list = ls())
```
1. Load the Stata data set **wages.dta**. The variables are `earnings` (in Euro, 2009), `age`, `gender` (male=1, female=2), `education` (years of education), `hours` (hours worked during
2009), and `weight`.
```{r}
library(foreign)
wages <- read.dta("data/wages.dta")
```

```{r}
head(wages)
earnings <- wages$earnings
age <- wages$age
gender <- wages$gender
education <- wages$education
hours <- wages$hours
weight <- wages$weight
```

+ Compute the (unweighted) wage equation
\begin{align*}
\ln \text{earnings}_{i}=\alpha +\beta _{1}\text{age}_{i}+\beta _{2}\text{age}_{i}^{2}+\beta _{3}\text{education}_{i}+\beta _{4}\text{gender}_{i}+u_{i},
\end{align*}
print the summary of the `lm`-object, and interpret the output.
```{r}
regr <- lm(log(earnings) ~ age + I(age^2) + education + gender)
summary(regr)
```

+ Add an interaction term for `education` and `gender` to the regression.
```{r}
regr2 <- lm(log(earnings) ~ age + I(age^2) + education + gender + education:gender) 
summary(regr2)
```

+ Compute the weighted hourly wage equation
\begin{align*}
\ln \frac{\text{earnings}_{i}}{\text{hours}_{i}}=\alpha +\beta _{1}\text{age}_{i}+\beta _{2}\text{age}_{i}^{2}+\beta _{3}\text{education}_{i}+\beta _{4}\text{gender}_{i}+u_{i},
\end{align*}
print the summary of the `lm`-object, and interpret the output.
```{r}
regr3 <- lm(log(earnings/hours) ~ age + I(age^2) + education + gender, weights = weight)
summary(regr3)
plot(regr3$residuals)
```

+ Activate the packages `lmtest` and `sandwich`. Use the function `coeftest` to compute the heteroskedasticity robust standard errors (`vcov=vcovHC`) for the estimated coefficients.
```{r}
library(lmtest)
library(sandwich)
```

```{r}
# Robust standard errors
coeftest(regr, vcov = vcovHC)
coeftest(regr2, vcov = vcovHC)
coeftest(regr3, vcov = vcovHC)
```


+ Predict the hourly wage of a male person aged 60 years as a function of education (vary the years of education between 9 and 18). Set the option `se.fit=TRUE`. Inspect the object returned by the `predict` command. Plot the predicted values and add the $\pm 2$ standard deviations confidence intervals.
```{r}
forecast <- predict(regr3, newdata = data.frame(education = seq(9, 18, by = 0.5), age = 60, gender = 1), se.fit = TRUE)
names(forecast)
plot(seq(9, 18, by = 0.5), forecast$fit, type = "l", lwd = 2)
lines(seq(9, 18, by = 0.5), forecast$fit + 2 * forecast$se.fit, type = "l", col = "red", lwd = 1.5)
lines(seq(9, 18, by = 0.5), forecast$fit - 2 * forecast$se.fit, type = "l", col = "red", lwd = 1.5)
```

2. Load the data set **bsp4.txt**.
```{r}
bsp4 <- read.csv("data/bsp4.txt")
head(bsp4)
```

+ Plot the scatter plot of $y$ against $x$.
```{r}
plot(bsp4$x, bsp4$y)
```

+ Perform a simple linear regression of $y$ on $x$ and save the results as an `lm`-object `obj`. Add the regression line of $y$ on $x$ to the plot.
```{r}
plot(bsp4$x, bsp4$y)
obj <- lm(bsp4$y ~ bsp4$x)
abline(obj)
```

+ Extract the fitted values from `obj` and add them as red points to the plot (use the command `points`).
```{r}
plot(bsp4$x, bsp4$y)
abline(obj)
points(bsp4$x, obj$fitted.values, col = "red")
```

+ Extract the residuals of the regression and calculate the sum of the squared residuals,
$SSR=\sum_{i=1}^{100}\hat{u}_{i}^{2}$
```{r}
ssr <- sum((obj$residuals)^2)
print(ssr)
```

+ Compute the total sum of squares and the explained sum of squares,
\begin{align*}
TSS &=\sum_{i=1}^{100}\left( y_{i}-\bar{y}\right) ^{2} \\
ESS &=\sum_{i=1}^{100}\left( \hat{y}_{i}-\bar{y}\right) ^{2}
\end{align*}
and show that $ESS+SSR=TSS$.
```{r}
tss <- sum((bsp4$y - mean(bsp4$y))^2)
ess <- sum((obj$fitted.values - mean(bsp4$y))^2)
ess + ssr - tss # this is numerically zero
round(ess + ssr) == round(tss)
```

